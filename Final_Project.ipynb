{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I am going to apply the deep learning knowledge to computer vision area. \n",
    "\n",
    "I would use the open source software library, TensorFlow, for my numercial computation. \n",
    "\n",
    "The dataset I would utlize for my studies is MNIST which is a simple computer vision dataset consisting of images of handwritten digits. I am going to train a model to look at images and predict what digits they are.<br>\n",
    "\n",
    "Bascially, I grasp the idea of building model from https://www.tensorflow.org/get_started/mnist/beginners. After building the sample model and sophisticated model along with the guideline, I get familiar with TensorFlow programming and also obtain a deeper understanding towards Convolutional Neural Network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESOURSES I learned before doing this final project:\n",
    "\n",
    "<b>1.</b>Deep Learning with Neural Networks and TensorFlow Introduction:\n",
    "https://www.youtube.com/watch?v=oYbVFhK_olY\n",
    "\n",
    "Help me review the basic knowledge of neuron networks including the introduction to single neuron, and the introduction to deep neural network. Also, the introduction to TensorFlow is also included in this video lecture.\n",
    "\n",
    "\n",
    "<b>2.</b>Getting started with Tensorflow:\n",
    "https://www.tensorflow.org/get_started/\n",
    "\n",
    "Berif overview of TensorFlow programming fundamentals, including introduction to MNIST for Machine Learning beginners.\n",
    "\n",
    "<b>3.</b>Convolutional Networks\n",
    "http://www.deeplearningbook.org/contents/convnets.html\n",
    "\n",
    "Detailed introduction to Convolutional Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What I Learn & What I Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first place, I would use the simple model, Softmax regression to dip a toe using TensorFlow. And I would apply neuron network to train a elaborate model in the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Model: Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "# Test Tensorflow, First Test\n",
    "hello = tf.constant('Hello, TensorFlow!');\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n",
    "\n",
    "# Test Tensorflow, Second Test\n",
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "print(sess.run(a+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes are used to download and read in MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Download and read in MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image in MNIST dataset is 28 pixels by 28 pixels, and we interpret the image into a big arrays of numbers and then faltten this array into a vector of 28x28=784 numbers. Then the test images of MNIST have been transformed into a bunch of points in a vector space with 784 dimensions.\n",
    "\n",
    "The code below generates a palceholder for input of value for everytime we ask TensorFlow for a computation. 'None' here means the length of a dimension is flexible, it can be any number. tf.float32 here means it is a 2-D tensor with floating-point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a placeholder for value of the input everytime we ask Tensorflow to run a computation\n",
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights and biases are needed in our modle. In this place, we use Variable in TensorFlow. Variable is a modifiable tensor of TensorFlow, it can be used and modified by the computation.\n",
    "\n",
    "Each image in MNIST dataset also maintains a label (0-9) indicating the digit drawn on the image. Therefore, the shape of W is [784,10] for generating a 10-dimensional vector of evidence used for indicating if the given image is in a particular number class. \n",
    "\n",
    "By using softmax regression, this kind of evidence can be transformed into a probability value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the weights and biases for the model\n",
    "# The initial values for weights and biases are zero since we are going to learn them so\n",
    "# it doesn't matter what they initially are.\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Implementing our model by using softmax regression\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training, we introduces a special Cross-entropy to determine the loss of the model.\n",
    "The expression of Cross-entropy can be shown like below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H_{y^{'}}(y)=-\\sum_{i}y_i^{'}log(y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we add another placeholder for storing the correct value read from MNIST dataset.\n",
    "Then based on the formula of Cross-entropy, we can get the code expression of Cross-entropy.\n",
    "The function <i><b>tf.nn.softmax_cross_entropy_with_logits</b></i> can also be used to calculate Cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add a new placeholder to input the correct answers\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Implementing cross_entropy function\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "#cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Gradient descent algorithm with a learning rate of 0.5 to minimize Cross-entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ask Tennsorflow to minimize the cross_entropy\n",
    "# The learning rate is 0.01\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this step, we have already set up our model. <br>\n",
    "Then We can launch our model in an InteractiveSession and we initial our variable in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lauch the model in an InteractiveSession called sess\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Create an operation to initialize the created variables\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run our training step 1000 times. For each time, we use stochastic training method to train which means taking  small batches of random data(100 points from our training set) for each step of loop. This kind of gradient descent is called stochastic gradient descent. The reason that you use this method is we'd like to use all our data for every step of training because that would give us a better sense of what we should be doing, but that's expensive. So, instead, we use a different subset every time. Doing this is cheap and has much of the same benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start to train the model by doing stochastic training\n",
    "for i in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our model, we use the function <i><b>tf.argmax</i></b><br>\n",
    "This function is used to get the index of the highest entry in a tensor along some axis. In our case, since the label of our test image is all\"one-hot vectors\" <b>(e.g. 4 would be [0,0,0,0,1,0,0,0,0,0])</b>, we just need to get the index of 1 in the vector.<br>\n",
    "Then we use the function <i><b>tf.equal</i></b> to check whether the label of our model is equal to the correct label. The type of correct_prediction is a list of boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use argmax function to see if the predicted value is equal to the corrected value\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see the accuracy of our model directly, we cast to floating point numbers and then take the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the correctness of the predictation\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can get the accuracy on our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9198\n"
     ]
    }
   ],
   "source": [
    "# At last, ask for the accuracy on our test data\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What I learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice, I use Softmax regression to build the simple model to train, the final accuracy we obtain is nearly 92% which seems high but actually it is not a good result. I do not get a ideal result because the model I use here is the simplest one. Through this simplest model, I get to know TensorFlow programming, I learned that TensorFlow is actually a library which maintains various functions. I remember when I did my homework, I had to code the gradient descent algorithm myself, which increase the amount and the complexity of codes. By using TensorFlow, we could use this algorithm directly by calling <i><b>tf.train.GradientDescentOptimizer</b></i> Besides, the usage of <i>Variable</i> is also wise. Besides, I get to know Cross-entropy, a new way for calculating the loss of a model. At last, the usage of stochastic training method is also a cheap way to utilize the all data of dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sophisticated Model: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf  \n",
    "import sys  \n",
    "from tensorflow.examples.tutorials.mnist import input_data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the initial value of weights, and the initial value is determined by the truncated normal distribution.\n",
    "The original expression of truncated normal distribution is :\n",
    "<i><b>tf.truncated_normal(shape, mean, stddev)</b></i>\n",
    "<br><b>shape</b> refers the shape of the tensor.\n",
    "<br>The default value of <b>mean</b> is zero.\n",
    "<br><b>stddev</b> is the standard deviation which is 0.1 in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):  \n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)  \n",
    "  return tf.Variable(initial)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the initial value of bias by calling <i><b>tf.constant(value,dtype=None,shape=None,name=’Const’)</b></i>\n",
    "<br> <b>value</b> can be a list or just a constant, it is 0.1 in the following cell.\n",
    "<br> <b>shape</b> is the shape of the tensor.\n",
    "<br> <b>dtype</b> and <b>name</b> can all be the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape):  \n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the function <i><b>tf.nn.conv2d</b></i> to do the convolution, the function of conv2d is:\n",
    "<br><i><b>conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None,data_format=None, name=None)</b></i>\n",
    "<br><b><i>input</i></b> is the data which is going to be convolved;\n",
    "<br><b><i>filter</i></b> is the convolutional kernel;\n",
    "<br><b><i>strides</i></b> is the list with the length of 4, which refers to the sliding distance of the convolutional window;\n",
    "<br><b><i>padding</i></b> can be \"SAME\" or \"VALID\" for indicating whether the edge of a image which can not be convolved is needed to be saved. \"SAME\" means saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):  \n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function <i><b>tf.nn.max_pool</i></b> is used to do the max pooling operation, the original function is:\n",
    "<br><i><b>max_pool(value, ksize, strides, padding, data_format=\"NHWC\", name=None)</b></i>\n",
    "<br>The format of <i><b>value</i></b> is the same as the format of <i><b>input</b></i> of <i><b>tf.nn.conv2d</b></i>, which is a tensor with 4 dimensions;\n",
    "<br><i><b>ksize</i></b> defines the shape of the pooling window,which should be a list with length of 4;\n",
    "<br><i><b>strides</i></b> is the sliding value of pooling window;\n",
    "<br><i><b>padding</i></b> is the same as the padding of convolutional function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):  \n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and read in the data of MNIST dataset.<br>\n",
    "Launch the model in an Interactive Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)  \n",
    "  \n",
    "sess = tf.InteractiveSession() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", shape=[None, 784])  \n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Layer<br>\n",
    "\n",
    "The shape of filter is 5`*`5, the number of input channel is 1, the number of output channel is 32;\n",
    "Reshape x to the shape of the input of <i><b>tf.conv2d</b></i>, -1 in <i><b>tf.reshape(x, [-1, 28, 28, 1])</b></i> means the number of input image is changeable.\n",
    "Therefore, after the first convolution, the size of the output should be x`*`28`*`28`*`32<br>\n",
    "During the pooling period, since <b>ksize=[1, 2, 2, 1]</b>, then the size of output is x`*`14`*`14`*`32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])  \n",
    "b_conv1 = bias_variable([32])  \n",
    "  \n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])  \n",
    "  \n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  \n",
    "h_pool1 = max_pool_2x2(h_conv1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Layer<br>\n",
    "\n",
    "The shape of filter is 5`*`5, the number of input channel is 32, the number of output channel is 64;\n",
    "Therefore, the size of input of the convolution is x`*`14`*`14`*`32, and the size of output of the convolution is x`*`14`*`14`*`64<br>\n",
    "After pooling, the size of output is x`*`7`*`7`*`64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])  \n",
    "b_conv2 = bias_variable([64])  \n",
    "  \n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)  \n",
    "h_pool2 = max_pool_2x2(h_conv2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third Layer<br>\n",
    "\n",
    "The layer is a Densely Connected Layer. The dimension of input is 7`*`7`*`64 and the dimension of output is 1024. We reshape the output of pooling operation, multiply weight and add the bias.<br>\n",
    "In here, the dropout function <i><b>tf.nn.dropout</i></b> is also used to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now image size is reduced to 7*7  \n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])  \n",
    "b_fc1 = bias_variable([1024])  \n",
    "  \n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])  \n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  \n",
    "  \n",
    "keep_prob = tf.placeholder(\"float\")  \n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forth Layer<br>\n",
    "\n",
    "This layer is a Readout Layer, which apply a layer of Softmax Regression just like the simple model we mentioned above. The dimension of input is 1024 and the dimension of output is 10, the output is the specific classification of labels (0 to 9). In here, Softmax is used as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])  \n",
    "b_fc2 = bias_variable([10])  \n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here we still choose to use Cross-entropy to determine the loss of the model. We replace the steepest gradient descent optimizer with the more sophisticated ADAM optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))  \n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the simple model, in this model, we still use the function <i><b>argmax</i></b> to get the index of the highest entry in a tensor along some axis. Besides, we include the additional parameter <i><b>keep_prob</b></i> in <i><b>feed_dict</b></i> to control the dropout rate. And we add logging to every 100th iteration in the training process.<br>\n",
    "We run 20000 training iterations, and we take 50 points from our dataset for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.140\n",
      "step 100, training accuracy 0.840\n",
      "step 200, training accuracy 0.940\n",
      "step 300, training accuracy 0.940\n",
      "step 400, training accuracy 0.980\n",
      "step 500, training accuracy 0.920\n",
      "step 600, training accuracy 0.960\n",
      "step 700, training accuracy 0.980\n",
      "step 800, training accuracy 0.980\n",
      "step 900, training accuracy 0.960\n",
      "step 1000, training accuracy 0.980\n",
      "step 1100, training accuracy 1.000\n",
      "step 1200, training accuracy 0.960\n",
      "step 1300, training accuracy 1.000\n",
      "step 1400, training accuracy 1.000\n",
      "step 1500, training accuracy 0.960\n",
      "step 1600, training accuracy 0.980\n",
      "step 1700, training accuracy 0.980\n",
      "step 1800, training accuracy 1.000\n",
      "step 1900, training accuracy 0.980\n",
      "step 2000, training accuracy 1.000\n",
      "step 2100, training accuracy 1.000\n",
      "step 2200, training accuracy 0.980\n",
      "step 2300, training accuracy 0.960\n",
      "step 2400, training accuracy 1.000\n",
      "step 2500, training accuracy 0.980\n",
      "step 2600, training accuracy 0.980\n",
      "step 2700, training accuracy 1.000\n",
      "step 2800, training accuracy 0.980\n",
      "step 2900, training accuracy 1.000\n",
      "step 3000, training accuracy 1.000\n",
      "step 3100, training accuracy 0.960\n",
      "step 3200, training accuracy 1.000\n",
      "step 3300, training accuracy 1.000\n",
      "step 3400, training accuracy 0.960\n",
      "step 3500, training accuracy 1.000\n",
      "step 3600, training accuracy 1.000\n",
      "step 3700, training accuracy 1.000\n",
      "step 3800, training accuracy 1.000\n",
      "step 3900, training accuracy 1.000\n",
      "step 4000, training accuracy 0.980\n",
      "step 4100, training accuracy 0.980\n",
      "step 4200, training accuracy 1.000\n",
      "step 4300, training accuracy 0.980\n",
      "step 4400, training accuracy 0.980\n",
      "step 4500, training accuracy 1.000\n",
      "step 4600, training accuracy 0.980\n",
      "step 4700, training accuracy 1.000\n",
      "step 4800, training accuracy 1.000\n",
      "step 4900, training accuracy 0.980\n",
      "step 5000, training accuracy 0.980\n",
      "step 5100, training accuracy 1.000\n",
      "step 5200, training accuracy 0.960\n",
      "step 5300, training accuracy 1.000\n",
      "step 5400, training accuracy 1.000\n",
      "step 5500, training accuracy 1.000\n",
      "step 5600, training accuracy 1.000\n",
      "step 5700, training accuracy 0.960\n",
      "step 5800, training accuracy 0.980\n",
      "step 5900, training accuracy 0.960\n",
      "step 6000, training accuracy 1.000\n",
      "step 6100, training accuracy 0.960\n",
      "step 6200, training accuracy 0.980\n",
      "step 6300, training accuracy 1.000\n",
      "step 6400, training accuracy 1.000\n",
      "step 6500, training accuracy 1.000\n",
      "step 6600, training accuracy 1.000\n",
      "step 6700, training accuracy 1.000\n",
      "step 6800, training accuracy 1.000\n",
      "step 6900, training accuracy 1.000\n",
      "step 7000, training accuracy 1.000\n",
      "step 7100, training accuracy 1.000\n",
      "step 7200, training accuracy 1.000\n",
      "step 7300, training accuracy 1.000\n",
      "step 7400, training accuracy 0.980\n",
      "step 7500, training accuracy 0.980\n",
      "step 7600, training accuracy 1.000\n",
      "step 7700, training accuracy 1.000\n",
      "step 7800, training accuracy 1.000\n",
      "step 7900, training accuracy 1.000\n",
      "step 8000, training accuracy 0.960\n",
      "step 8100, training accuracy 0.980\n",
      "step 8200, training accuracy 1.000\n",
      "step 8300, training accuracy 1.000\n",
      "step 8400, training accuracy 1.000\n",
      "step 8500, training accuracy 1.000\n",
      "step 8600, training accuracy 0.980\n",
      "step 8700, training accuracy 1.000\n",
      "step 8800, training accuracy 1.000\n",
      "step 8900, training accuracy 1.000\n",
      "step 9000, training accuracy 1.000\n",
      "step 9100, training accuracy 1.000\n",
      "step 9200, training accuracy 0.980\n",
      "step 9300, training accuracy 1.000\n",
      "step 9400, training accuracy 1.000\n",
      "step 9500, training accuracy 1.000\n",
      "step 9600, training accuracy 0.980\n",
      "step 9700, training accuracy 1.000\n",
      "step 9800, training accuracy 1.000\n",
      "step 9900, training accuracy 1.000\n",
      "step 10000, training accuracy 1.000\n",
      "step 10100, training accuracy 1.000\n",
      "step 10200, training accuracy 1.000\n",
      "step 10300, training accuracy 0.980\n",
      "step 10400, training accuracy 1.000\n",
      "step 10500, training accuracy 1.000\n",
      "step 10600, training accuracy 1.000\n",
      "step 10700, training accuracy 1.000\n",
      "step 10800, training accuracy 0.980\n",
      "step 10900, training accuracy 1.000\n",
      "step 11000, training accuracy 0.980\n",
      "step 11100, training accuracy 1.000\n",
      "step 11200, training accuracy 1.000\n",
      "step 11300, training accuracy 1.000\n",
      "step 11400, training accuracy 0.980\n",
      "step 11500, training accuracy 1.000\n",
      "step 11600, training accuracy 1.000\n",
      "step 11700, training accuracy 1.000\n",
      "step 11800, training accuracy 1.000\n",
      "step 11900, training accuracy 1.000\n",
      "step 12000, training accuracy 1.000\n",
      "step 12100, training accuracy 1.000\n",
      "step 12200, training accuracy 1.000\n",
      "step 12300, training accuracy 1.000\n",
      "step 12400, training accuracy 0.980\n",
      "step 12500, training accuracy 1.000\n",
      "step 12600, training accuracy 1.000\n",
      "step 12700, training accuracy 1.000\n",
      "step 12800, training accuracy 1.000\n",
      "step 12900, training accuracy 1.000\n",
      "step 13000, training accuracy 1.000\n",
      "step 13100, training accuracy 1.000\n",
      "step 13200, training accuracy 1.000\n",
      "step 13300, training accuracy 1.000\n",
      "step 13400, training accuracy 1.000\n",
      "step 13500, training accuracy 1.000\n",
      "step 13600, training accuracy 0.980\n",
      "step 13700, training accuracy 1.000\n",
      "step 13800, training accuracy 1.000\n",
      "step 13900, training accuracy 0.980\n",
      "step 14000, training accuracy 1.000\n",
      "step 14100, training accuracy 1.000\n",
      "step 14200, training accuracy 1.000\n",
      "step 14300, training accuracy 1.000\n",
      "step 14400, training accuracy 1.000\n",
      "step 14500, training accuracy 1.000\n",
      "step 14600, training accuracy 1.000\n",
      "step 14700, training accuracy 1.000\n",
      "step 14800, training accuracy 1.000\n",
      "step 14900, training accuracy 1.000\n",
      "step 15000, training accuracy 1.000\n",
      "step 15100, training accuracy 1.000\n",
      "step 15200, training accuracy 1.000\n",
      "step 15300, training accuracy 1.000\n",
      "step 15400, training accuracy 1.000\n",
      "step 15500, training accuracy 1.000\n",
      "step 15600, training accuracy 1.000\n",
      "step 15700, training accuracy 1.000\n",
      "step 15800, training accuracy 1.000\n",
      "step 15900, training accuracy 1.000\n",
      "step 16000, training accuracy 1.000\n",
      "step 16100, training accuracy 1.000\n",
      "step 16200, training accuracy 1.000\n",
      "step 16300, training accuracy 1.000\n",
      "step 16400, training accuracy 1.000\n",
      "step 16500, training accuracy 1.000\n",
      "step 16600, training accuracy 1.000\n",
      "step 16700, training accuracy 1.000\n",
      "step 16800, training accuracy 1.000\n",
      "step 16900, training accuracy 1.000\n",
      "step 17000, training accuracy 1.000\n",
      "step 17100, training accuracy 1.000\n",
      "step 17200, training accuracy 1.000\n",
      "step 17300, training accuracy 1.000\n",
      "step 17400, training accuracy 1.000\n",
      "step 17500, training accuracy 1.000\n",
      "step 17600, training accuracy 1.000\n",
      "step 17700, training accuracy 1.000\n",
      "step 17800, training accuracy 1.000\n",
      "step 17900, training accuracy 1.000\n",
      "step 18000, training accuracy 1.000\n",
      "step 18100, training accuracy 1.000\n",
      "step 18200, training accuracy 1.000\n",
      "step 18300, training accuracy 1.000\n",
      "step 18400, training accuracy 1.000\n",
      "step 18500, training accuracy 1.000\n",
      "step 18600, training accuracy 1.000\n",
      "step 18700, training accuracy 1.000\n",
      "step 18800, training accuracy 0.980\n",
      "step 18900, training accuracy 1.000\n",
      "step 19000, training accuracy 1.000\n",
      "step 19100, training accuracy 1.000\n",
      "step 19200, training accuracy 1.000\n",
      "step 19300, training accuracy 1.000\n",
      "step 19400, training accuracy 1.000\n",
      "step 19500, training accuracy 1.000\n",
      "step 19600, training accuracy 1.000\n",
      "step 19700, training accuracy 1.000\n",
      "step 19800, training accuracy 1.000\n",
      "step 19900, training accuracy 1.000\n",
      "Training finished\n",
      "test accuracy 0.992\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))  \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))  \n",
    "sess.run(tf.global_variables_initializer()) \n",
    "\n",
    "for i in range(20000):  \n",
    "  batch = mnist.train.next_batch(50)  \n",
    "  if i%100 == 0:  \n",
    "    train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})  \n",
    "    print (\"step %d, training accuracy %.3f\"%(i, train_accuracy)) \n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})  \n",
    "  \n",
    "print (\"Training finished\") \n",
    "  \n",
    "print (\"test accuracy %.3f\" % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What I learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose to apply convolutional neural network to realize the sophisticated model. By building the model, I get to know the principle of CNN. The operation of CNN maintains four steps: InputImage, Convolution, MaxPooling and the calculation of Fully-ConnectedNeural Network. By realizing each step one by one, I get to know the implemantation of each step including which functions of TensorFlow should I use to build and train the model. Therefore, I further acquire the knowledge related to TensorFlow. In my ponit of view, the usage of the function <i><b>tf.nn.dropout</b></i> is ingenious，in this place, a placeholder for the probability that a neuron's output is kept during dropout. This allows us to turn dropout on during training, and turn it off during testing. Also, <i><b>tf.nn.dropout</b></i> operation automatically handles scaling neuron outputs in addition to masking them, so dropout just works without any additional scaling.<br>\n",
    "Besides, the usage of <i><b>tf.train.AdamOptimizer</b></i> is also appropriate. Adam is an optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. Stochastic gradient descent that we used in simple model maintains a single learning rate but Adam computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. By using Adam, we could achieve good results fast and efficiently solve practical deep learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the final test accuracy of two models, we can clearly see that the accuracy of the simple Softmax regression model is nearly 92% and the test accuracy of the CNN sophisticated model reached 99% at the end of 20000 iterations which we can say the model built based on CNN maintains a better training result than the simple model built based on Softmax Regression.<br>\n",
    "\n",
    "Softmax regression or we can say the logistic regression takes one input variable to calculate the probability of something else, it is a great, robust model for simple classification tasks. When the size of dataset is relatively very small, we can use logistic regression for the classification. But in many situations, the number of inputs can be various and these input variables can be weighed in different ways, thus Softmax regression can no longer satisfies the requirement. The multiclassification problem should be sloved by using neural network for it has the advantage to learn more complex, non-linear functions. <br>\n",
    "\n",
    "Besides, when deciding the best values of the parameters of two algorithms, usually gradient descent is used in logistic regression to minimize the cost function and on each iteration better values to fit their data. In Neural Networks, a different way can be used. We can use other algorithms to find the parameters (or weights) to our inputs, like Adam optimization algorithm is used in the sophisticated model.<br>\n",
    "\n",
    "But at the same time, on neural networks, we still need to use Softmax or other function to find the probability that that specific input is of a class 1,2,..,K, just like the last layer of CNN in my sophisticated model. In this way, we can say that the neural networks is kind of improvement of logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
